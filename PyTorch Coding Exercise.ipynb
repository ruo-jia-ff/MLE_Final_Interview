{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean Distance, Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def euclidean_distance(stacked_tensor):\n",
    "    reduced_tensor = stacked_tensor[:, :, 0, :] - stacked_tensor[:, :, 1, :]\n",
    "    tensor_squared = reduced_tensor**2\n",
    "    target = tensor_squared.sum()\n",
    "    target = torch.sqrt(target)\n",
    "    return target\n",
    "\n",
    "class EuclideanModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EuclideanModel, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=1, kernel_size=(2,1), bias = False)       \n",
    "        self.fc = None\n",
    "\n",
    "    def activation(self, x):\n",
    "        return x**2\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        conv_out = self.conv(x)    \n",
    "        conv_out = self.activation(conv_out)\n",
    "        conv_out_flat = conv_out.reshape(conv_out.size(0), -1)\n",
    "\n",
    "        self.fc = None\n",
    "        tensor_width = conv_out.reshape(conv_out.size(0), -1).size(1)\n",
    "        self.fc = nn.Linear(tensor_width, 1, bias=False)\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient tracking for manual weight assignment\n",
    "            self.fc.weight.fill_(1)\n",
    "    \n",
    "        output = self.fc(conv_out_flat)\n",
    "        output = torch.sqrt(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch: 100%|██████████| 1000/1000 [00:04<00:00, 248.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are the weights: Parameter containing:\n",
      "tensor([[[[ 0.9990],\n",
      "          [-0.9987]]]], requires_grad=True)\n",
      "THIS is the lOSS: 0.006159065291285515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from torch.optim.lr_scheduler import StepLR, LambdaLR\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "torch.manual_seed(2)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "trained_model = EuclideanModel()\n",
    "\n",
    "# Generate random tensors to train\n",
    "num_samples = 50\n",
    "batch_size = 50\n",
    "vector_dim = 15\n",
    "\n",
    "mean = 2\n",
    "std = 3.9\n",
    "data = mean + std * torch.randn(num_samples, 2, 15)\n",
    "data = data.unsqueeze(1)\n",
    "data.shape\n",
    "\n",
    "optimizer = optim.AdamW(trained_model.parameters(), lr=0.01)\n",
    "\n",
    "# Construct a learning rate lambda\n",
    "def lr_lambda(batch_idx):\n",
    "\n",
    "    decay_interval = 2\n",
    "    decay_rate = 0.9\n",
    "    if batch_idx % decay_interval == 0 and batch_idx > 0:\n",
    "        return decay_rate\n",
    "    return 1\n",
    "\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# Create data loader\n",
    "dataset = TensorDataset(data)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Number of epochs for training\n",
    "epochs = 1000\n",
    "\n",
    "# Training loop\n",
    "for epoch in tqdm(range(epochs), desc = \"Training Epoch: \"):\n",
    "    for batch_idx, (inputs,) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = trained_model(inputs)\n",
    "        \n",
    "        # Assuming the targets are the same as the inputs (for simplicity)\n",
    "        targets = torch.stack([euclidean_distance(inputs[i:i+1]) for i in range(inputs.size(0))]).view(batch_size, 1)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)*10\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        loss.backward()        # Backpropagate the loss\n",
    "        optimizer.step()       # Update model parameters\n",
    "        scheduler.step()\n",
    "        \n",
    "print(f\"These are the weights: {trained_model.conv.weight}\")\n",
    "print(f\"THIS is the lOSS: {loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The error relative to true value is 0.007527828216552734\n"
     ]
    }
   ],
   "source": [
    "# Test performance against a random tensor \n",
    "dim = 15\n",
    "input_tensor = torch.cat((torch.randn(1, 1, 1, dim), torch.randn(1, 1, 1, dim)), dim=2)\n",
    "\n",
    "error = abs(trained_model(input_tensor) - euclidean_distance(input_tensor)).cpu().detach().numpy()[0][0]\n",
    "print(f\"The error relative to true value is {error}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
